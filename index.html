<!DOCTYPE html>
<html lang="en-us">

<head>
  <meta charset="UTF-8">
  <title>Breast Cancer Prediction</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
  <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
</head>

<body>
  <section class="page-header">
    <h1 class="project-name">Breast Cancer Prediction</h1>
    <h2 class="project-tagline">Summer 2020: Wong Kin Li, Chanelleah Miller</h2>

  </section>

  <section class="main-content">
    <h2>
      <a id="welcome-to-github-pages" class="anchor" aria-hidden="true"><span aria-hidden="true"
          class="octicon octicon-link"></span></a>Introduction</h2>
    <h4>Motivation</h4>
    <p>The second major cause of women's death is breast cancer (after lung cancer) [1]. 246,660 of women's new cases of
      invasive breast cancer are expected to be diagnosed in the US during 2016 and 40,450 of women’s death is estimated
      [2]. Breast cancer represents about 12% of all new cancer cases and 25% of all cancers in women [3]. </p>
    <p>On the other hand, advancements in machine learning have simplified the process of cancer identification,
      enabling doctors to make a diagnosis before the onset of common symptoms [2]. As more sophisticated tools are
      developed, the diagnosis of breast cancer can be made more accurately[3]. A machine learning based diagnosis of
      malignant breast cancer cells can allow doctors to make a more accurate diagnosis of the patients based on the
      tissue cells. This reduces both the cost of such medical tests error rate in identifying breast cancer in
      patients. This ultimately saves more lives.</p>
    <p>Therefore in this project, we aim to use a combination of unsupervised and supervised learning to weigh different
      features of a digitized image of a fine needle aspirate (FNA) of a breast mass, which describe characteristics of
      the cell nuclei present in the image, in order to accurately predict if the cell is indeed malignant. This
      eventually assist doctors in the diagnosis of breast cancer in patients. Ultimately, we hope to use the fitted
      model as baseline diagnostic tools for patients. The analysis below outlines each of the methods used, leading up
      to our final recommendations.</p>
    <h4>Dataset</h4>
    <p>The UCI Breast Cancer Dataset [5] is a public data set that has its features computed from a digitized image of a
      breast mass. This data set covers the relationship between the malignancy of a cancer cell and its physical
      properties. The dataset consists of 10 features, 1 binary label, and 569 training examples.</p>
    <p>Ten real-valued features are computed for each cell nucleus taken from the breast tissue:</p>
    <ul style="list-style-type:disc;">
      <li>radius (mean of distances from center to points on the perimeter)</li>
      <li>texture (standard deviation of gray-scale values)</li>
      <li>perimeter</li>
      <li>area</li>
      <li>smoothness (local variation in radius lengths)</li>
      <li>compactness (perimeter^2 / area - 1.0)</li>
      <li>concavity (severity of concave portions of the contour)</li>
      <li>concave points (number of concave portions of the contour)</li>
      <li>symmetry</li>
      <li>fractal dimension ("coastline approximation" - 1)</li>

    </ul>
    <h4>Correlation Matrix</h4>
    <img src="images/correlation.png" width="600" height="450">
    <p>In general, we know that a correlation coefficient of more than (absolute value of) 0.7 may suggest
      multicollinearity between features. After plotting the correlation matrix, we can see that some of the
      features are highly correlated. The correlation coefficient is in the value of 0.8 to 1.0. Since some of the
      features are related to one another physically (e.g. area, radius, and perimeter), it is unsurprising to see
      such a result of the correlation matrix. </p>

    <h2>
      <a id="designer-templates" class="anchor" aria-hidden="true"><span aria-hidden="true"
          class="octicon octicon-link"></span></a>Feature Distributions</h2>
    <img src="images/distribution.png" width="500" height="600">
    <p>Since all ten features are continuous, we plotted their histogram graphs to investigate how these features are
      distributed. Looking at the distributions of all ten continuous features, we can see that they are mostly normally
      distributed. Some of the features are skewed to the left, while some are skewed to the right.</p>

    <h2>
      <a id="designer-templates" class="anchor" aria-hidden="true"><span aria-hidden="true"
          class="octicon octicon-link"></span></a>Kernel Density Estimation</h2>
    <img src="images/kde.png" width="350" height="500">
    <p>We also use Kernel Density Estimation to estimate the pdf of the continuous variables. The result reflects what
      we had before as they all seem to be normally distributed.</p>


    <h2>
      <a id="designer-templates" class="anchor" aria-hidden="true"><span aria-hidden="true"
          class="octicon octicon-link"></span></a>K-Means Algorithm</h2>
    <img src="images/kmeans1.png" width="300" height="300">
    <p>To figure out what subset of the features we should run the K-Means algorithm on, we first plotted the data based
      by its features. The data is shown with each of the 10 different features against the condition - show as 0 if
      breast tissue cell is benign and 1 if it is malignant.
    </p>
    <img src="images/kmeans2.png" width="300" height="300">
    <p>The first method looked rather inconclusive and did not provide valuable insight. Therefore, we explored the
      second method: plot the data points with each of the features against one another to see which pair gives us
      interesting plots. Each of these data points is shown as purple if the cell is benign or red if the cell is malignant.
      After looking at these graphs, we find that all the attribute pairs are, in fact, worth exploring. We therefore
      decided to run the K-means algorithm on all of them.
    </p>

    <img src="images/kmeans3.png" width="400" height="800">
    <p>After running the K-means algorithms on all 45 pairs of attribute relationships, we graphed the number of
      clusters against the distortion value in an attempt to use the elbow method to see the optimal number of clusters
      to use for each pair. The motivation here was to understand the spread of the data in the dataset but this
      experiment did not provide valuable insights. It might be more useful with a larger dataset, where sample sizes
      are at least more than a few thousand.
    </p>


    <h2>
      <a id="designer-templates" class="anchor" aria-hidden="true"><span aria-hidden="true"
          class="octicon octicon-link"></span></a>Principle Component Analysis</h2>
    <img src="images/pca.png" width="389" height="278">
    <p>Through PCA analysis, we aimed to find out an optimal number of attributes that would yield good results for our
      later algorithms. Based on the graph, there seems to be a plateau between 6 and 8 that suggests the optimal number
      of features. It could possibly be explained by the previous insight that some of the features are correlated with
      one another physically and thus might not be useful to include them.</p>

    <h2>
      <a id="designer-templates" class="anchor" aria-hidden="true"><span aria-hidden="true"
          class="octicon octicon-link"></span></a>Random Forest</h2>
    <img src="images/dtree1.png" width="355" height="262">
    <p>To train and test our data, we chose a random 33% split between training and test data. Producing a random forest
      without any dimension reduction, the accuracy was very high – 94.6%.</p>

    <img src="images/dtree2.png" width="384" height="264">
    <p>Next, we wanted to see how the prediction accuracy of our Random Forest would be affected by dimensional reduction.
      We calculated the feature importance of each feature and ranked them. We then fit the model using only the
      top 5 features. This produced a lower accuracy of 92.5%. Thus, the dimensional reduction is not useful in this case.
    </p>
    <h2>
      <a id="designer-templates" class="anchor" aria-hidden="true"><span aria-hidden="true"
          class="octicon octicon-link"></span></a>Logistic Regression</h2>
    <img src="images/logistics1.png" width="356" height="263">
    <p>Next, we trained a logistic regression model which gave us an accuracy of 0.894 or 89.4% accuracy.</p>

    <h2>
      <a id="designer-templates" class="anchor" aria-hidden="true"><span aria-hidden="true"
          class="octicon octicon-link"></span></a>Naives Bay</h2>
    <img src="images/naivesbay.png" width="356" height="263">
    <p>Last but not least, we applied Naives Bay algorithm to the dataset and found a low prediction accuracy of 0.665,
      compared to other models.
      This low accuracy rate could come from the false assumption in this model that all the features are independent of
      each other in order to interpret each features'
      conditional probability in contributing to the presence of cancer. However, we know from earlier analysis that
      some of the features
      are not in fact independent of one another.
    </p>

    <h2>
      <a id="designer-templates" class="anchor" aria-hidden="true"><span aria-hidden="true"
          class="octicon octicon-link"></span></a>Conclusion</h2>
    <h4>Discussion</h4>
    <table style="width:40%">
      <tr>
        <th>Model</th>
        <th>Accuracy</th>
      </tr>
      <tr>
        <td>Random Forest</td>
        <td>0.94149</td>

      </tr>
      <tr>
        <td>Logistics Regression</td>
        <td>0.89362</td>
      </tr>
      <tr>
        <td>Naives Bayes</td>
        <td>0.66489</td>
      </tr>
    </table>
    <p>Based on the values we see in the table, the method that yields the best and most accurate predictions was random
      forest. Similarly, the prediction accuracy of logistic regression comes close. Both results could be due to the
      possibly linear relationship from some of the features~~. This also explains why Naives Baye did not
      work well for this dataset since the independence assumption cannot hold in this case.
    </p>
    <h4>Future Steps</h4>
    <p>In the future, we hope to have a larger data set with more training examples to prevent overfitting when training
      our models. This lack of data could account for the less-than-optimal accuracy even in our best model. </p>
    <p>The future of breast cancer diagnosis is bright – the accuracy will only improve. Even with few hundred of
      training examples, our methods show multiple methods to make a good diagnosis of patients based on their breast
      tissue cells. By increasing the accuracy of these machine learning models and fine-tuning certain hyperparameters,
      we believe the breast cancer can be caught earlier and tackled better worldwide.</p>
    <h2>
      <a id="designer-templates" class="anchor" aria-hidden="true"><span aria-hidden="true"
          class="octicon octicon-link"></span></a>References</h2>
    <ol>
      <li>Asri, H., Mousannif, H., Al Moatassime, H. and Noel, T., 2016. Using machine learning algorithms for breast
        cancer risk prediction and diagnosis. Procedia Computer Science, 83, pp.1064-1069.</li>
      <li>Key, T., Verkasalo, P. and Banks, E., 2001. Epidemiology of breast cancer. The Lancet Oncology, 2(3),
        pp.133-140.</li>
      <li>Liu, H., Zhang, R., Luan, F., Yao, X., Liu, M., Hu, Z. and Fan, B., 2003. Diagnosing Breast Cancer Based on
        Support Vector Machines. ChemInform, 34(34).</li>
      <li>Jerez, J., Molina, I., García-Laencina, P., Alba, E., Ribelles, N., Martín, M. and Franco, L., 2010. Missing
        data imputation using statistical and machine learning methods in a real breast cancer problem. Artificial
        Intelligence in Medicine, 50(2), pp.105-115.</li>
      <li>Kaggle.com. 2020. Breast Cancer Wisconsin (Diagnostic) Data Set. [online] Available at:
        <https://www.kaggle.com/uciml/breast-cancer-wisconsin-data> [Accessed 12 June 2020].</li> </ol> <footer
          class="site-footer">
          <span class="site-footer-owner"><a
              href="https://github.gatech.edu/wli455/cs4641-final-project">Cs4641-final-project</a> is maintained by <a
              href="https://github.gatech.edu/wli455">wli455</a>.</span>

          <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub
              Pages</a>
            using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a
              href="https://twitter.com/jasonlong">Jason Long</a>.</span>
          </footer>

  </section>


</body>

</html>